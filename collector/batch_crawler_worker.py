# -*- coding: utf-8 -*-
"""å‡çº§ç‰ˆå°çº¢ä¹¦å•†å“é‡‡é›†Worker - æ”¯æŒæ‰¹é‡é“¾æ¥ã€çŸ­é“¾æ¥ã€å†å²è®°å½•"""
import os
import re
import time
import requests
import json
from datetime import datetime

BASE_URL = os.getenv("WEBSPIDER_API", "http://localhost:1337/api")
WORKER_ID = os.getenv("WEBSPIDER_WORKER_ID", "batch-crawler-worker")
CLAIM_LIMIT = int(os.getenv("WEBSPIDER_CLAIM_LIMIT", "1"))


def extract_urls_from_text(text):
    """ä»å¤šè¡Œæ–‡æœ¬ä¸­æå–æ‰€æœ‰é“¾æ¥

    æ”¯æŒæ ¼å¼:
    1. å®Œæ•´é“¾æ¥: https://www.xiaohongshu.com/goods-detail/677deacf627fd90001933466?xsec_token=...
    2. çŸ­é“¾æ¥: https://xhslink.com/m/4ZhKf0O6l4h
    3. æ–‡æœ¬åˆ†äº«: ã€å°çº¢ä¹¦ã€‘æ ‡é¢˜ ğŸ˜† çŸ­ç  ğŸ˜† https://xhslink.com/m/xxx ...
    """
    urls = []

    # æŒ‰è¡Œåˆ†å‰²
    lines = text.strip().split('\n')

    for line in lines:
        line = line.strip()
        if not line:
            continue

        # æå–æ‰€æœ‰URL
        # åŒ¹é… https://www.xiaohongshu.com/goods-detail/... æˆ– https://xhslink.com/m/...
        url_patterns = [
            r'https://www\.xiaohongshu\.com/goods-detail/[a-zA-Z0-9\?&=\-_%]+',
            r'https://xhslink\.com/m/[a-zA-Z0-9]+'
        ]

        for pattern in url_patterns:
            matches = re.findall(pattern, line)
            urls.extend(matches)

    # å»é‡
    return list(set(urls))


def resolve_short_url(short_url):
    """é‡å®šå‘çŸ­é“¾æ¥è·å–çœŸå®item_id

    çŸ­é“¾æ¥å¦‚: https://xhslink.com/m/4ZhKf0O6l4h
    ä¼šé‡å®šå‘åˆ°: https://www.xiaohongshu.com/goods-detail/673071e618193500011e6dd0?...
    """
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        }
        response = requests.get(short_url, headers=headers, timeout=10, allow_redirects=True)

        # ä»æœ€ç»ˆURLä¸­æå–item_id
        final_url = response.url
        print(f"[REDIRECT] {short_url} -> {final_url}")

        match = re.search(r'goods-detail/([a-zA-Z0-9]+)', final_url)
        if match:
            return match.group(1)
        return None
    except Exception as e:
        print(f"[ERROR] Failed to resolve short URL: {e}")
        return None


def extract_item_id(url):
    """ä»å•†å“URLä¸­æå–item_id"""
    # å¦‚æœæ˜¯çŸ­é“¾æ¥,å…ˆé‡å®šå‘
    if 'xhslink.com' in url:
        return resolve_short_url(url)

    # å®Œæ•´é“¾æ¥ç›´æ¥æå–
    match = re.search(r'goods-detail/([a-zA-Z0-9]+)', url)
    if match:
        return match.group(1)
    return None


def fetch_product_data(item_id):
    """è°ƒç”¨å°çº¢ä¹¦APIè·å–çœŸå®å•†å“æ•°æ®"""
    api_url = f"https://mall.xiaohongshu.com/api/store/jpd/edith/detail/h5/toc"
    params = {
        'version': '0.0.5',
        'item_id': item_id
    }

    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Referer': 'https://www.xiaohongshu.com/',
    }

    try:
        response = requests.get(api_url, params=params, headers=headers, timeout=10)
        response.raise_for_status()
        data = response.json()

        if not data.get('success'):
            print(f"[ERROR] API returned error: {data.get('msg')}")
            return None

        return data.get('data')
    except Exception as e:
        print(f"[ERROR] Failed to fetch product data: {e}")
        return None


def parse_product_data(data):
    """è§£æAPIè¿”å›çš„å•†å“æ•°æ®"""
    if not data or 'template_data' not in data:
        return None

    template_data = data['template_data']
    if not template_data or len(template_data) == 0:
        return None

    info = template_data[0]

    # æå–å•†å“ä¿¡æ¯
    result = {
        'title': None,
        'price': None,
        'sales': None,
        'seller_name': None,
        'seller_id': None,
        'cover_url': None,
        'description': None,
        'images': []
    }

    # æ ‡é¢˜å’ŒåŸºæœ¬ä¿¡æ¯
    if 'descriptionH5' in info:
        desc = info['descriptionH5']
        result['title'] = desc.get('name')
        result['item_id'] = desc.get('skuId')

    # ä»·æ ¼ä¿¡æ¯ (å•ä½æ˜¯åˆ†,éœ€è¦é™¤ä»¥100)
    if 'priceH5' in info:
        price_info = info['priceH5']
        highlight_price = price_info.get('highlightPrice')
        if highlight_price:
            result['price'] = float(highlight_price)

        # é”€é‡ä¿¡æ¯
        sales_text = price_info.get('itemAnalysisDataText', '')
        sales_match = re.search(r'å·²å”®(\d+)', sales_text)
        if sales_match:
            result['sales'] = int(sales_match.group(1))

    # ä»·æ ¼ä¿¡æ¯(ä»bottomBarMainH5è·å–,å•ä½ä¹Ÿæ˜¯åˆ†)
    if 'bottomBarMainH5' in info:
        bottom_bar = info['bottomBarMainH5']
        price = bottom_bar.get('price')
        if price:
            result['price'] = float(price) / 100

    # å–å®¶ä¿¡æ¯
    if 'sellerH5' in info:
        seller = info['sellerH5']
        result['seller_name'] = seller.get('name')
        result['seller_id'] = seller.get('id')
        result['seller_logo'] = seller.get('logo')
        result['seller_fans'] = seller.get('fansAmount')
        result['seller_sales'] = seller.get('salesVolume')

    # å•†å“å›¾ç‰‡
    if 'carouselH5' in info:
        carousel = info['carouselH5']
        images = carousel.get('images', [])
        result['images'] = [img.get('url') for img in images if img.get('url')]
        if result['images']:
            result['cover_url'] = 'https:' + result['images'][0] if result['images'][0].startswith('//') else result['images'][0]

    # å•†å“æè¿°
    if 'graphicDetailsV4' in info:
        graphic = info['graphicDetailsV4']
        result['description'] = graphic.get('description')

    # ç‰©æµä¿¡æ¯
    if 'goodsDistributeV4' in info:
        distribute = info['goodsDistributeV4']
        result['location'] = distribute.get('location')
        result['shipping_fee'] = distribute.get('fee')

    return result


def save_product_history(product_data, product_url, task_id):
    """ä¿å­˜å•†å“å†å²è®°å½•"""
    history_data = {
        "data": {
            "productId": product_data.get('item_id'),
            "title": product_data.get('title'),
            "sellerName": product_data.get('seller_name'),
            "totalSales": product_data.get('sales') or 0,
            "price": product_data.get('price') or 0,
            "sales": product_data.get('sales') or 0,
            "collectTime": datetime.now().isoformat(),
            "productUrl": product_url,
            "accountUrl": f"https://www.xiaohongshu.com/user/profile/{product_data.get('seller_id')}" if product_data.get('seller_id') else None,
            "crawlTask": task_id
        }
    }

    try:
        response = requests.post(
            f"{BASE_URL}/product-histories",
            json=history_data,
            timeout=10
        )
        response.raise_for_status()
        print(f"[HISTORY] Saved product history for {product_data.get('item_id')}")
    except Exception as e:
        print(f"[WARN] Failed to save product history: {e}")


def claim_tasks():
    """é¢†å–ä»»åŠ¡"""
    response = requests.post(
        f"{BASE_URL}/crawl-tasks/actions/claim",
        json={"workerId": WORKER_ID, "limit": CLAIM_LIMIT},
        timeout=10,
    )
    response.raise_for_status()
    payload = response.json()
    return payload.get("data", [])


def submit_result(task, product_data, product_url):
    """æäº¤çœŸå®çš„å•†å“æ•°æ®"""
    task_id = task["id"]

    # æå–ä»»åŠ¡æ•°æ®(å…¼å®¹Strapi 5)
    if "attributes" in task:
        task_data = task["attributes"]
    else:
        task_data = task

    # æ„å»ºå•†å“æ•°æ®
    product_payload = {
        "externalId": product_data.get('item_id') or task_data.get('url'),
        "title": product_data.get('title') or 'Unknown Product',
        "price": product_data.get('price') or 0,
        "sales": product_data.get('sales') or 0,
        "coverUrl": product_data.get('cover_url'),
        "metadata": {
            "description": product_data.get('description'),
            "images": product_data.get('images', []),
            "location": product_data.get('location'),
            "shipping_fee": product_data.get('shipping_fee'),
            "crawled_at": time.time(),
        }
    }

    # åº—é“ºæ•°æ®
    if product_data.get('seller_id'):
        product_payload["store"] = {
            "externalId": product_data['seller_id'],
            "name": product_data.get('seller_name') or 'Unknown Store',
            "logoUrl": product_data.get('seller_logo'),
            "metadata": {
                "fans": product_data.get('seller_fans'),
                "sales_volume": product_data.get('seller_sales'),
            }
        }

    # ä¿å­˜å†å²è®°å½•
    save_product_history(product_data, product_url, task_id)

    # æäº¤å®ŒæˆçŠ¶æ€
    response = requests.post(
        f"{BASE_URL}/crawl-tasks/{task_id}/actions/complete",
        json={
            "state": "done",
            "product": product_payload,
            "result": {"raw": product_data},
        },
        timeout=10,
    )
    response.raise_for_status()
    print(f"[OK] Task #{task_id} completed - {product_data.get('title')}")
    print(f"     Price: {product_data.get('price')} | Sales: {product_data.get('sales')}")


def process_single_url(url, task):
    """å¤„ç†å•ä¸ªURL"""
    print(f"\n[URL] Processing: {url}")

    # æå–item_id
    item_id = extract_item_id(url)
    if not item_id:
        print(f"[ERROR] Cannot extract item_id from URL: {url}")
        return False

    print(f"[CRAWL] Fetching product data for item_id: {item_id}")

    # è·å–çœŸå®å•†å“æ•°æ®
    api_data = fetch_product_data(item_id)
    if not api_data:
        print(f"[ERROR] Failed to fetch product data")
        return False

    # è§£æå•†å“æ•°æ®
    product_data = parse_product_data(api_data)
    if not product_data:
        print(f"[ERROR] Failed to parse product data")
        return False

    print(f"[PARSED] Title: {product_data.get('title')}")
    print(f"[PARSED] Price: {product_data.get('price')} yuan")
    print(f"[PARSED] Sales: {product_data.get('sales')}")
    print(f"[PARSED] Seller: {product_data.get('seller_name')}")

    # æäº¤ç»“æœ
    try:
        submit_result(task, product_data, url)
        return True
    except Exception as e:
        print(f"[ERROR] Failed to submit result: {e}")
        return False


def run_once():
    """è¿è¡Œä¸€æ¬¡é‡‡é›†è½®è¯¢"""
    tasks = claim_tasks()
    if not tasks:
        print("[WAIT] No tasks available, sleeping...")
        time.sleep(5)
        return

    print(f"[CLAIM] Claimed {len(tasks)} task(s)")

    for task in tasks:
        task_id = task["id"]

        # å…¼å®¹Strapi 5æ•°æ®ç»“æ„
        if "attributes" in task:
            task_url = task["attributes"].get("url")
            task_title = task["attributes"].get("title", "Untitled")
        else:
            task_url = task.get("url")
            task_title = task.get("title", "Untitled")

        print(f"\n[PROCESS] Task #{task_id} - {task_title}")

        # æå–æ‰€æœ‰URL
        urls = extract_urls_from_text(task_url)
        if not urls:
            print(f"[ERROR] No valid URLs found in task")
            continue

        print(f"[BATCH] Found {len(urls)} URL(s) to process")

        # å¤„ç†æ¯ä¸ªURL
        success_count = 0
        for idx, url in enumerate(urls, 1):
            print(f"\n[{idx}/{len(urls)}] Processing URL...")
            if process_single_url(url, task):
                success_count += 1
            # å»¶è¿Ÿé¿å…APIé™åˆ¶
            if idx < len(urls):
                time.sleep(2)

        print(f"\n[SUMMARY] Task #{task_id}: {success_count}/{len(urls)} URLs processed successfully")


if __name__ == "__main__":
    print("=" * 60)
    print("[START] Batch XHS Product Crawler Worker")
    print(f"[CONFIG] Worker ID: {WORKER_ID}")
    print(f"[CONFIG] API URL: {BASE_URL}")
    print(f"[CONFIG] Claim limit: {CLAIM_LIMIT}")
    print("=" * 60)

    while True:
        try:
            run_once()
        except Exception as exc:
            print(f"[WARN] Worker error: {exc}")
            import traceback
            traceback.print_exc()
            time.sleep(10)
